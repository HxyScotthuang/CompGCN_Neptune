{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbStjOS20/nbhuclH6mxjs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVgnB8Yxhppa","executionInfo":{"status":"ok","timestamp":1678921653858,"user_tz":0,"elapsed":19303,"user":{"displayName":"scott huang","userId":"06489357941762676089"}},"outputId":"65c3b3eb-00c2-4d7f-d739-037be2f1e5b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune\")"],"metadata":{"id":"xN2qsn71hvya","executionInfo":{"status":"ok","timestamp":1678921670075,"user_tz":0,"elapsed":921,"user":{"displayName":"scott huang","userId":"06489357941762676089"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install Cython\n","!pip install ordered_set\n","!pip install --pre -U scikit_learn\n","!pip install torch_scatter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhyIaYAQh56H","executionInfo":{"status":"ok","timestamp":1678922802811,"user_tz":0,"elapsed":752170,"user":{"displayName":"scott huang","userId":"06489357941762676089"}},"outputId":"e4a9c8e2-1ede-44ca-e27a-e8fc685de06d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: Cython in /usr/local/lib/python3.9/dist-packages (0.29.33)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ordered_set in /usr/local/lib/python3.9/dist-packages (3.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit_learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit_learn) (1.22.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit_learn) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit_learn) (3.1.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit_learn) (1.10.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_scatter\n","  Downloading torch_scatter-2.1.0.tar.gz (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: torch_scatter\n","  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_scatter: filename=torch_scatter-2.1.0-cp39-cp39-linux_x86_64.whl size=3556607 sha256=1859244ea7d694851361c29f4b4a0a2bdf0a9af43d81ab2bbdd5bbf5d47ddb97\n","  Stored in directory: /root/.cache/pip/wheels/c5/33/3c/b02defb8e41252b9073b3b98433e082a8fb9aa8945127ffcbe\n","Successfully built torch_scatter\n","Installing collected packages: torch_scatter\n","Successfully installed torch_scatter-2.1.0\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install neptune-client"],"metadata":{"id":"HTXwVl0qmMVm","executionInfo":{"status":"ok","timestamp":1678922836087,"user_tz":0,"elapsed":26754,"user":{"displayName":"scott huang","userId":"06489357941762676089"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!python run.py -score_func distmult -opn mult -gcn_layer 3 -gpu 0 -data FB15k-237 -name FB \\\n","    -logdir '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/log/' \\\n","    -config '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/config/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWWESv_Hh-s7","outputId":"c84a7540-19a3-432b-d2c3-674bb4240b87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.9/dist-packages/neptune/internal/backends/hosted_client.py:48: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n","  from neptune.version import version as neptune_client_version\n","/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/run.py:6: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n","  import neptune.new as neptune\n","https://app.neptune.ai/R-NBFNet/CMPNN-Rebuttal/e/CMPNNREB-26\n","{'name': 'FB_16_03_2023_00:55:18', 'dataset': 'FB15k-237', 'model': 'compgcn', 'score_func': 'distmult', 'opn': 'mult', 'batch_size': 128, 'gamma': 40.0, 'gpu': '0', 'max_epochs': 500, 'l2': 0.0, 'lr': 0.001, 'lbl_smooth': 0.1, 'num_workers': 10, 'seed': 41504, 'restore': False, 'bias': False, 'num_bases': -1, 'init_dim': 100, 'gcn_dim': 200, 'embed_dim': None, 'gcn_layer': 3, 'dropout': 0.1, 'hid_drop': 0.3, 'hid_drop2': 0.3, 'feat_drop': 0.3, 'k_w': 10, 'k_h': 20, 'num_filt': 200, 'ker_sz': 7, 'log_dir': '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/log/', 'config_dir': '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/config/'}\n","/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/run.py:476: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n","        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n","        for dictionaries or collections that contain unsupported values.\n","        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n","  run[\"params\"] = params\n","2023-03-16 00:55:18,558 - [INFO] - {'name': 'FB_16_03_2023_00:55:18', 'dataset': 'FB15k-237', 'model': 'compgcn', 'score_func': 'distmult', 'opn': 'mult', 'batch_size': 128, 'gamma': 40.0, 'gpu': '0', 'max_epochs': 500, 'l2': 0.0, 'lr': 0.001, 'lbl_smooth': 0.1, 'num_workers': 10, 'seed': 41504, 'restore': False, 'bias': False, 'num_bases': -1, 'init_dim': 100, 'gcn_dim': 200, 'embed_dim': None, 'gcn_layer': 3, 'dropout': 0.1, 'hid_drop': 0.3, 'hid_drop2': 0.3, 'feat_drop': 0.3, 'k_w': 10, 'k_h': 20, 'num_filt': 200, 'ker_sz': 7, 'log_dir': '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/log/', 'config_dir': '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/config/'}\n","{'batch_size': 128,\n"," 'bias': False,\n"," 'config_dir': '/content/drive/MyDrive/Colab '\n","               'Notebooks/partC/CompGCN_Neptune/config/',\n"," 'dataset': 'FB15k-237',\n"," 'dropout': 0.1,\n"," 'embed_dim': None,\n"," 'feat_drop': 0.3,\n"," 'gamma': 40.0,\n"," 'gcn_dim': 200,\n"," 'gcn_layer': 3,\n"," 'gpu': '0',\n"," 'hid_drop': 0.3,\n"," 'hid_drop2': 0.3,\n"," 'init_dim': 100,\n"," 'k_h': 20,\n"," 'k_w': 10,\n"," 'ker_sz': 7,\n"," 'l2': 0.0,\n"," 'lbl_smooth': 0.1,\n"," 'log_dir': '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/log/',\n"," 'lr': 0.001,\n"," 'max_epochs': 500,\n"," 'model': 'compgcn',\n"," 'name': 'FB_16_03_2023_00:55:18',\n"," 'num_bases': -1,\n"," 'num_filt': 200,\n"," 'num_workers': 10,\n"," 'opn': 'mult',\n"," 'restore': False,\n"," 'score_func': 'distmult',\n"," 'seed': 41504}\n","2023-03-16 00:55:24,940 - [INFO] - [E:0| 0]: Train Loss:0.6932,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:27,453 - [INFO] - [E:0| 100]: Train Loss:0.57833,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:29,938 - [INFO] - [E:0| 200]: Train Loss:0.37816,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:32,404 - [INFO] - [E:0| 300]: Train Loss:0.27492,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:34,867 - [INFO] - [E:0| 400]: Train Loss:0.21244,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:37,341 - [INFO] - [E:0| 500]: Train Loss:0.17286,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:39,790 - [INFO] - [E:0| 600]: Train Loss:0.14577,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:42,252 - [INFO] - [E:0| 700]: Train Loss:0.12592,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:44,719 - [INFO] - [E:0| 800]: Train Loss:0.11078,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:47,175 - [INFO] - [E:0| 900]: Train Loss:0.099057,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:49,644 - [INFO] - [E:0| 1000]: Train Loss:0.08953,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:52,105 - [INFO] - [E:0| 1100]: Train Loss:0.081861,  Val MRR:0.0\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:53,864 - [INFO] - [Epoch:0]:  Training Loss:0.07728\n","\n","/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/run.py:321: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n","  pred \t\t\t= torch.where(label.byte(), -torch.ones_like(pred) * 10000000, pred)\n","2023-03-16 00:55:54,285 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:55,518 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:56,478 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:57,706 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:55:58,207 - [INFO] - [Epoch 0 valid]: MRR: Tail : 0.08582, Head : 0.00792, Avg : 0.04687\n","2023-03-16 00:55:58,304 - [INFO] - [Epoch 0]: Training Loss: 0.077279, Valid MRR: 0.04687\n","\n","\n","2023-03-16 00:55:58,798 - [INFO] - [E:1| 0]: Train Loss:0.0032623,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:01,264 - [INFO] - [E:1| 100]: Train Loss:0.0041293,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:03,726 - [INFO] - [E:1| 200]: Train Loss:0.0037479,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:06,201 - [INFO] - [E:1| 300]: Train Loss:0.0035746,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:08,674 - [INFO] - [E:1| 400]: Train Loss:0.0035596,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:11,146 - [INFO] - [E:1| 500]: Train Loss:0.0034976,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:13,624 - [INFO] - [E:1| 600]: Train Loss:0.0034202,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:16,117 - [INFO] - [E:1| 700]: Train Loss:0.003343,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:18,602 - [INFO] - [E:1| 800]: Train Loss:0.0032856,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:21,068 - [INFO] - [E:1| 900]: Train Loss:0.0032521,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:23,551 - [INFO] - [E:1| 1000]: Train Loss:0.0032543,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:26,020 - [INFO] - [E:1| 1100]: Train Loss:0.0032104,  Val MRR:0.04687\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:27,805 - [INFO] - [Epoch:1]:  Training Loss:0.003194\n","\n","2023-03-16 00:56:28,238 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:29,466 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:30,422 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:31,683 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:32,184 - [INFO] - [Epoch 1 valid]: MRR: Tail : 0.14499, Head : 0.02166, Avg : 0.08333\n","2023-03-16 00:56:32,304 - [INFO] - [Epoch 1]: Training Loss: 0.0031943, Valid MRR: 0.08333\n","\n","\n","2023-03-16 00:56:32,751 - [INFO] - [E:2| 0]: Train Loss:0.0023654,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:35,233 - [INFO] - [E:2| 100]: Train Loss:0.0027019,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:37,698 - [INFO] - [E:2| 200]: Train Loss:0.0027155,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:40,173 - [INFO] - [E:2| 300]: Train Loss:0.0026964,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:42,618 - [INFO] - [E:2| 400]: Train Loss:0.0026866,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:45,108 - [INFO] - [E:2| 500]: Train Loss:0.0027191,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:47,600 - [INFO] - [E:2| 600]: Train Loss:0.0027161,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:50,062 - [INFO] - [E:2| 700]: Train Loss:0.0026927,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:52,536 - [INFO] - [E:2| 800]: Train Loss:0.0026895,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:55,009 - [INFO] - [E:2| 900]: Train Loss:0.0026649,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:57,495 - [INFO] - [E:2| 1000]: Train Loss:0.0026491,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:56:59,986 - [INFO] - [E:2| 1100]: Train Loss:0.0026348,  Val MRR:0.08333\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:01,766 - [INFO] - [Epoch:2]:  Training Loss:0.002631\n","\n","2023-03-16 00:57:02,217 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:03,412 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:04,342 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:05,563 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:06,052 - [INFO] - [Epoch 2 valid]: MRR: Tail : 0.16535, Head : 0.0378, Avg : 0.10158\n","2023-03-16 00:57:06,167 - [INFO] - [Epoch 2]: Training Loss: 0.0026305, Valid MRR: 0.10158\n","\n","\n","2023-03-16 00:57:06,652 - [INFO] - [E:3| 0]: Train Loss:0.0023762,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:09,128 - [INFO] - [E:3| 100]: Train Loss:0.0024828,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:11,611 - [INFO] - [E:3| 200]: Train Loss:0.0024813,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:14,093 - [INFO] - [E:3| 300]: Train Loss:0.0025019,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:16,559 - [INFO] - [E:3| 400]: Train Loss:0.0024951,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:19,040 - [INFO] - [E:3| 500]: Train Loss:0.0024895,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:21,515 - [INFO] - [E:3| 600]: Train Loss:0.0024831,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:23,976 - [INFO] - [E:3| 700]: Train Loss:0.0024748,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:26,480 - [INFO] - [E:3| 800]: Train Loss:0.0024587,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:29,002 - [INFO] - [E:3| 900]: Train Loss:0.0024578,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:31,496 - [INFO] - [E:3| 1000]: Train Loss:0.0024496,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:33,965 - [INFO] - [E:3| 1100]: Train Loss:0.0024458,  Val MRR:0.10158\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:35,748 - [INFO] - [Epoch:3]:  Training Loss:0.00244\n","\n","2023-03-16 00:57:36,169 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:37,401 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:38,360 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:39,637 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:40,151 - [INFO] - [Epoch 3 valid]: MRR: Tail : 0.21293, Head : 0.04376, Avg : 0.12834\n","2023-03-16 00:57:40,278 - [INFO] - [Epoch 3]: Training Loss: 0.00244, Valid MRR: 0.12834\n","\n","\n","2023-03-16 00:57:40,757 - [INFO] - [E:4| 0]: Train Loss:0.0021065,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:43,277 - [INFO] - [E:4| 100]: Train Loss:0.0022893,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:45,772 - [INFO] - [E:4| 200]: Train Loss:0.0022953,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:48,272 - [INFO] - [E:4| 300]: Train Loss:0.0023208,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:50,759 - [INFO] - [E:4| 400]: Train Loss:0.0023315,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:53,242 - [INFO] - [E:4| 500]: Train Loss:0.0023353,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:55,736 - [INFO] - [E:4| 600]: Train Loss:0.0023317,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:57:58,224 - [INFO] - [E:4| 700]: Train Loss:0.0023381,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:00,707 - [INFO] - [E:4| 800]: Train Loss:0.0023291,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:03,206 - [INFO] - [E:4| 900]: Train Loss:0.0023238,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:05,691 - [INFO] - [E:4| 1000]: Train Loss:0.0023221,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:08,185 - [INFO] - [E:4| 1100]: Train Loss:0.0023185,  Val MRR:0.12834\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:10,006 - [INFO] - [Epoch:4]:  Training Loss:0.002319\n","\n","2023-03-16 00:58:10,444 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:11,707 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:12,674 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:13,917 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:14,419 - [INFO] - [Epoch 4 valid]: MRR: Tail : 0.23722, Head : 0.0647, Avg : 0.15096\n","2023-03-16 00:58:14,525 - [INFO] - [Epoch 4]: Training Loss: 0.0023187, Valid MRR: 0.15096\n","\n","\n","2023-03-16 00:58:15,004 - [INFO] - [E:5| 0]: Train Loss:0.0020983,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:17,489 - [INFO] - [E:5| 100]: Train Loss:0.0022577,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:19,982 - [INFO] - [E:5| 200]: Train Loss:0.0022377,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:22,464 - [INFO] - [E:5| 300]: Train Loss:0.0022267,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:24,951 - [INFO] - [E:5| 400]: Train Loss:0.0022534,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:27,468 - [INFO] - [E:5| 500]: Train Loss:0.0022323,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:29,955 - [INFO] - [E:5| 600]: Train Loss:0.0022302,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:32,454 - [INFO] - [E:5| 700]: Train Loss:0.0022186,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:34,949 - [INFO] - [E:5| 800]: Train Loss:0.0022169,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:37,454 - [INFO] - [E:5| 900]: Train Loss:0.0022137,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:39,950 - [INFO] - [E:5| 1000]: Train Loss:0.0022069,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:42,442 - [INFO] - [E:5| 1100]: Train Loss:0.0022083,  Val MRR:0.15096\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:44,232 - [INFO] - [Epoch:5]:  Training Loss:0.002208\n","\n","2023-03-16 00:58:44,672 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:45,903 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:46,861 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:48,128 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:48,636 - [INFO] - [Epoch 5 valid]: MRR: Tail : 0.2701, Head : 0.0909, Avg : 0.1805\n","2023-03-16 00:58:48,754 - [INFO] - [Epoch 5]: Training Loss: 0.0022084, Valid MRR: 0.1805\n","\n","\n","2023-03-16 00:58:49,244 - [INFO] - [E:6| 0]: Train Loss:0.0024733,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:51,740 - [INFO] - [E:6| 100]: Train Loss:0.0021112,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:54,243 - [INFO] - [E:6| 200]: Train Loss:0.0021768,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:56,729 - [INFO] - [E:6| 300]: Train Loss:0.0021685,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:58:59,200 - [INFO] - [E:6| 400]: Train Loss:0.0021536,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:01,697 - [INFO] - [E:6| 500]: Train Loss:0.0021521,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:04,210 - [INFO] - [E:6| 600]: Train Loss:0.0021598,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:06,704 - [INFO] - [E:6| 700]: Train Loss:0.0021581,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:09,193 - [INFO] - [E:6| 800]: Train Loss:0.0021472,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:11,692 - [INFO] - [E:6| 900]: Train Loss:0.0021376,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:14,182 - [INFO] - [E:6| 1000]: Train Loss:0.0021355,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:16,691 - [INFO] - [E:6| 1100]: Train Loss:0.0021316,  Val MRR:0.1805\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:18,477 - [INFO] - [Epoch:6]:  Training Loss:0.002131\n","\n","2023-03-16 00:59:18,881 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:20,116 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:21,053 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:22,331 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:22,829 - [INFO] - [Epoch 6 valid]: MRR: Tail : 0.28069, Head : 0.10549, Avg : 0.19309\n","2023-03-16 00:59:22,943 - [INFO] - [Epoch 6]: Training Loss: 0.0021313, Valid MRR: 0.19309\n","\n","\n","2023-03-16 00:59:23,410 - [INFO] - [E:7| 0]: Train Loss:0.0017701,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:25,885 - [INFO] - [E:7| 100]: Train Loss:0.0020925,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:28,367 - [INFO] - [E:7| 200]: Train Loss:0.0021128,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:30,851 - [INFO] - [E:7| 300]: Train Loss:0.0020861,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:33,326 - [INFO] - [E:7| 400]: Train Loss:0.002062,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:35,789 - [INFO] - [E:7| 500]: Train Loss:0.0020665,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:38,254 - [INFO] - [E:7| 600]: Train Loss:0.0020581,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:40,731 - [INFO] - [E:7| 700]: Train Loss:0.0020749,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:43,193 - [INFO] - [E:7| 800]: Train Loss:0.0020643,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:45,675 - [INFO] - [E:7| 900]: Train Loss:0.0020676,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:48,150 - [INFO] - [E:7| 1000]: Train Loss:0.0020616,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:50,658 - [INFO] - [E:7| 1100]: Train Loss:0.0020502,  Val MRR:0.19309\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:52,456 - [INFO] - [Epoch:7]:  Training Loss:0.002048\n","\n","2023-03-16 00:59:52,882 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:54,130 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:55,056 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:56,330 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:56,831 - [INFO] - [Epoch 7 valid]: MRR: Tail : 0.31366, Head : 0.12801, Avg : 0.22084\n","2023-03-16 00:59:56,952 - [INFO] - [Epoch 7]: Training Loss: 0.0020481, Valid MRR: 0.22084\n","\n","\n","2023-03-16 00:59:57,459 - [INFO] - [E:8| 0]: Train Loss:0.0017434,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 00:59:59,914 - [INFO] - [E:8| 100]: Train Loss:0.0020298,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:02,372 - [INFO] - [E:8| 200]: Train Loss:0.0020138,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:04,863 - [INFO] - [E:8| 300]: Train Loss:0.0020285,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:07,327 - [INFO] - [E:8| 400]: Train Loss:0.0020218,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:09,777 - [INFO] - [E:8| 500]: Train Loss:0.0020253,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:12,240 - [INFO] - [E:8| 600]: Train Loss:0.0020226,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:14,693 - [INFO] - [E:8| 700]: Train Loss:0.002014,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:17,142 - [INFO] - [E:8| 800]: Train Loss:0.0020111,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:19,608 - [INFO] - [E:8| 900]: Train Loss:0.0020099,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:22,096 - [INFO] - [E:8| 1000]: Train Loss:0.0020072,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:24,577 - [INFO] - [E:8| 1100]: Train Loss:0.0020008,  Val MRR:0.22084\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:26,362 - [INFO] - [Epoch:8]:  Training Loss:0.001994\n","\n","2023-03-16 01:00:26,793 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:28,015 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:28,959 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:30,237 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:30,737 - [INFO] - [Epoch 8 valid]: MRR: Tail : 0.32868, Head : 0.14146, Avg : 0.23507\n","2023-03-16 01:00:30,846 - [INFO] - [Epoch 8]: Training Loss: 0.0019935, Valid MRR: 0.23507\n","\n","\n","2023-03-16 01:00:31,345 - [INFO] - [E:9| 0]: Train Loss:0.010983,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:33,813 - [INFO] - [E:9| 100]: Train Loss:0.0020219,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:36,310 - [INFO] - [E:9| 200]: Train Loss:0.001998,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:38,805 - [INFO] - [E:9| 300]: Train Loss:0.0019632,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:41,279 - [INFO] - [E:9| 400]: Train Loss:0.0019532,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:43,739 - [INFO] - [E:9| 500]: Train Loss:0.0019549,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:46,188 - [INFO] - [E:9| 600]: Train Loss:0.0019422,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:48,661 - [INFO] - [E:9| 700]: Train Loss:0.0019315,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:51,118 - [INFO] - [E:9| 800]: Train Loss:0.0019236,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:53,616 - [INFO] - [E:9| 900]: Train Loss:0.0019168,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:56,074 - [INFO] - [E:9| 1000]: Train Loss:0.0019273,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:00:58,533 - [INFO] - [E:9| 1100]: Train Loss:0.0019258,  Val MRR:0.23507\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:00,313 - [INFO] - [Epoch:9]:  Training Loss:0.001927\n","\n","2023-03-16 01:01:00,729 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:01,968 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:02,913 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:04,199 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:04,707 - [INFO] - [Epoch 9 valid]: MRR: Tail : 0.34153, Head : 0.14631, Avg : 0.24392\n","2023-03-16 01:01:04,832 - [INFO] - [Epoch 9]: Training Loss: 0.0019266, Valid MRR: 0.24392\n","\n","\n","2023-03-16 01:01:05,325 - [INFO] - [E:10| 0]: Train Loss:0.0016202,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:07,804 - [INFO] - [E:10| 100]: Train Loss:0.0018876,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:10,274 - [INFO] - [E:10| 200]: Train Loss:0.0018838,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:12,721 - [INFO] - [E:10| 300]: Train Loss:0.0018719,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:15,178 - [INFO] - [E:10| 400]: Train Loss:0.0018752,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:17,623 - [INFO] - [E:10| 500]: Train Loss:0.0018794,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:20,100 - [INFO] - [E:10| 600]: Train Loss:0.001895,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:22,568 - [INFO] - [E:10| 700]: Train Loss:0.0018851,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:25,024 - [INFO] - [E:10| 800]: Train Loss:0.0018835,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:27,479 - [INFO] - [E:10| 900]: Train Loss:0.0018827,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:29,955 - [INFO] - [E:10| 1000]: Train Loss:0.0018808,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:32,440 - [INFO] - [E:10| 1100]: Train Loss:0.0018727,  Val MRR:0.24392\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:34,226 - [INFO] - [Epoch:10]:  Training Loss:0.001871\n","\n","2023-03-16 01:01:34,672 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:35,930 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:36,876 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:38,097 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:38,605 - [INFO] - [Epoch 10 valid]: MRR: Tail : 0.35198, Head : 0.16042, Avg : 0.2562\n","2023-03-16 01:01:38,716 - [INFO] - [Epoch 10]: Training Loss: 0.0018709, Valid MRR: 0.2562\n","\n","\n","2023-03-16 01:01:39,173 - [INFO] - [E:11| 0]: Train Loss:0.0019258,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:41,641 - [INFO] - [E:11| 100]: Train Loss:0.0018128,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:44,096 - [INFO] - [E:11| 200]: Train Loss:0.0018508,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:46,551 - [INFO] - [E:11| 300]: Train Loss:0.0018378,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:49,033 - [INFO] - [E:11| 400]: Train Loss:0.0018275,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:51,494 - [INFO] - [E:11| 500]: Train Loss:0.0018406,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:53,980 - [INFO] - [E:11| 600]: Train Loss:0.0018371,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:56,454 - [INFO] - [E:11| 700]: Train Loss:0.001836,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:01:58,920 - [INFO] - [E:11| 800]: Train Loss:0.001841,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:01,378 - [INFO] - [E:11| 900]: Train Loss:0.0018394,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:03,878 - [INFO] - [E:11| 1000]: Train Loss:0.0018383,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:06,368 - [INFO] - [E:11| 1100]: Train Loss:0.0018409,  Val MRR:0.2562\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:08,164 - [INFO] - [Epoch:11]:  Training Loss:0.001836\n","\n","2023-03-16 01:02:08,595 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:09,834 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:10,793 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:12,035 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:12,541 - [INFO] - [Epoch 11 valid]: MRR: Tail : 0.36434, Head : 0.16298, Avg : 0.26366\n","2023-03-16 01:02:12,666 - [INFO] - [Epoch 11]: Training Loss: 0.0018361, Valid MRR: 0.26366\n","\n","\n","2023-03-16 01:02:13,194 - [INFO] - [E:12| 0]: Train Loss:0.0016917,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:15,677 - [INFO] - [E:12| 100]: Train Loss:0.0018689,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:18,147 - [INFO] - [E:12| 200]: Train Loss:0.0018636,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:20,628 - [INFO] - [E:12| 300]: Train Loss:0.0018553,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:23,122 - [INFO] - [E:12| 400]: Train Loss:0.0018607,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:25,601 - [INFO] - [E:12| 500]: Train Loss:0.0018426,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:28,104 - [INFO] - [E:12| 600]: Train Loss:0.0018276,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:30,600 - [INFO] - [E:12| 700]: Train Loss:0.0018225,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:33,087 - [INFO] - [E:12| 800]: Train Loss:0.0018216,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:35,575 - [INFO] - [E:12| 900]: Train Loss:0.0018139,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:38,072 - [INFO] - [E:12| 1000]: Train Loss:0.0018093,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:40,569 - [INFO] - [E:12| 1100]: Train Loss:0.0018073,  Val MRR:0.26366\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:42,349 - [INFO] - [Epoch:12]:  Training Loss:0.001808\n","\n","2023-03-16 01:02:42,805 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:44,048 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:45,016 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:46,269 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:46,769 - [INFO] - [Epoch 12 valid]: MRR: Tail : 0.37132, Head : 0.17296, Avg : 0.27214\n","2023-03-16 01:02:46,883 - [INFO] - [Epoch 12]: Training Loss: 0.0018077, Valid MRR: 0.27214\n","\n","\n","2023-03-16 01:02:47,375 - [INFO] - [E:13| 0]: Train Loss:0.0015899,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:49,844 - [INFO] - [E:13| 100]: Train Loss:0.0017551,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:52,318 - [INFO] - [E:13| 200]: Train Loss:0.0017959,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:54,770 - [INFO] - [E:13| 300]: Train Loss:0.0017778,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:57,254 - [INFO] - [E:13| 400]: Train Loss:0.0017615,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:02:59,724 - [INFO] - [E:13| 500]: Train Loss:0.001761,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:02,189 - [INFO] - [E:13| 600]: Train Loss:0.0017568,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:04,642 - [INFO] - [E:13| 700]: Train Loss:0.0017566,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:07,107 - [INFO] - [E:13| 800]: Train Loss:0.0017602,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:09,573 - [INFO] - [E:13| 900]: Train Loss:0.0017664,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:12,051 - [INFO] - [E:13| 1000]: Train Loss:0.0017712,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:14,510 - [INFO] - [E:13| 1100]: Train Loss:0.0017722,  Val MRR:0.27214\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:16,269 - [INFO] - [Epoch:13]:  Training Loss:0.001773\n","\n","2023-03-16 01:03:16,706 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:17,949 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:18,907 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:20,129 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:20,613 - [INFO] - [Epoch 13 valid]: MRR: Tail : 0.37299, Head : 0.18142, Avg : 0.27721\n","2023-03-16 01:03:20,707 - [INFO] - [Epoch 13]: Training Loss: 0.0017726, Valid MRR: 0.27721\n","\n","\n","2023-03-16 01:03:21,193 - [INFO] - [E:14| 0]: Train Loss:0.0016818,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:23,685 - [INFO] - [E:14| 100]: Train Loss:0.0018076,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:26,160 - [INFO] - [E:14| 200]: Train Loss:0.0017842,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:28,602 - [INFO] - [E:14| 300]: Train Loss:0.0017853,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:31,064 - [INFO] - [E:14| 400]: Train Loss:0.0017774,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:33,516 - [INFO] - [E:14| 500]: Train Loss:0.0017702,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:35,981 - [INFO] - [E:14| 600]: Train Loss:0.0017622,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:38,470 - [INFO] - [E:14| 700]: Train Loss:0.0017605,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:40,952 - [INFO] - [E:14| 800]: Train Loss:0.0017608,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:43,428 - [INFO] - [E:14| 900]: Train Loss:0.0017512,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:45,917 - [INFO] - [E:14| 1000]: Train Loss:0.0017487,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:48,419 - [INFO] - [E:14| 1100]: Train Loss:0.001746,  Val MRR:0.27721\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:50,202 - [INFO] - [Epoch:14]:  Training Loss:0.001745\n","\n","2023-03-16 01:03:50,642 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:51,875 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:52,841 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:54,074 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:54,567 - [INFO] - [Epoch 14 valid]: MRR: Tail : 0.38136, Head : 0.18686, Avg : 0.28411\n","2023-03-16 01:03:54,674 - [INFO] - [Epoch 14]: Training Loss: 0.0017451, Valid MRR: 0.28411\n","\n","\n","2023-03-16 01:03:55,154 - [INFO] - [E:15| 0]: Train Loss:0.0018447,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:03:57,619 - [INFO] - [E:15| 100]: Train Loss:0.0017593,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:00,091 - [INFO] - [E:15| 200]: Train Loss:0.0017292,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:02,548 - [INFO] - [E:15| 300]: Train Loss:0.0017246,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:05,016 - [INFO] - [E:15| 400]: Train Loss:0.0017308,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:07,473 - [INFO] - [E:15| 500]: Train Loss:0.0017399,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:09,942 - [INFO] - [E:15| 600]: Train Loss:0.001734,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:12,406 - [INFO] - [E:15| 700]: Train Loss:0.0017395,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:14,888 - [INFO] - [E:15| 800]: Train Loss:0.0017379,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:17,363 - [INFO] - [E:15| 900]: Train Loss:0.0017343,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:19,843 - [INFO] - [E:15| 1000]: Train Loss:0.001735,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:22,319 - [INFO] - [E:15| 1100]: Train Loss:0.0017369,  Val MRR:0.28411\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:24,114 - [INFO] - [Epoch:15]:  Training Loss:0.001734\n","\n","2023-03-16 01:04:24,543 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:25,809 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:26,781 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:27,994 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:28,496 - [INFO] - [Epoch 15 valid]: MRR: Tail : 0.38813, Head : 0.18659, Avg : 0.28736\n","2023-03-16 01:04:28,609 - [INFO] - [Epoch 15]: Training Loss: 0.0017341, Valid MRR: 0.28736\n","\n","\n","2023-03-16 01:04:29,093 - [INFO] - [E:16| 0]: Train Loss:0.0016135,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:31,562 - [INFO] - [E:16| 100]: Train Loss:0.0017771,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:34,026 - [INFO] - [E:16| 200]: Train Loss:0.0017068,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:36,497 - [INFO] - [E:16| 300]: Train Loss:0.0016989,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:38,961 - [INFO] - [E:16| 400]: Train Loss:0.0017135,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:41,415 - [INFO] - [E:16| 500]: Train Loss:0.0017087,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:43,875 - [INFO] - [E:16| 600]: Train Loss:0.0017093,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:46,332 - [INFO] - [E:16| 700]: Train Loss:0.0017115,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:48,805 - [INFO] - [E:16| 800]: Train Loss:0.0017114,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:51,264 - [INFO] - [E:16| 900]: Train Loss:0.001713,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:53,726 - [INFO] - [E:16| 1000]: Train Loss:0.0017176,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:56,189 - [INFO] - [E:16| 1100]: Train Loss:0.0017139,  Val MRR:0.28736\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:57,959 - [INFO] - [Epoch:16]:  Training Loss:0.001715\n","\n","2023-03-16 01:04:58,382 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:04:59,625 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:00,569 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:01,854 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:02,362 - [INFO] - [Epoch 16 valid]: MRR: Tail : 0.38917, Head : 0.1914, Avg : 0.29029\n","2023-03-16 01:05:02,473 - [INFO] - [Epoch 16]: Training Loss: 0.001715, Valid MRR: 0.29029\n","\n","\n","2023-03-16 01:05:02,934 - [INFO] - [E:17| 0]: Train Loss:0.0015766,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:05,436 - [INFO] - [E:17| 100]: Train Loss:0.0016745,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:07,926 - [INFO] - [E:17| 200]: Train Loss:0.0016892,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:10,414 - [INFO] - [E:17| 300]: Train Loss:0.0016971,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:12,913 - [INFO] - [E:17| 400]: Train Loss:0.001708,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:15,382 - [INFO] - [E:17| 500]: Train Loss:0.001702,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:17,850 - [INFO] - [E:17| 600]: Train Loss:0.0016946,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:20,314 - [INFO] - [E:17| 700]: Train Loss:0.001687,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:22,788 - [INFO] - [E:17| 800]: Train Loss:0.0016901,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:25,276 - [INFO] - [E:17| 900]: Train Loss:0.0016862,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:27,778 - [INFO] - [E:17| 1000]: Train Loss:0.0016987,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:30,256 - [INFO] - [E:17| 1100]: Train Loss:0.0016981,  Val MRR:0.29029\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:32,033 - [INFO] - [Epoch:17]:  Training Loss:0.001698\n","\n","2023-03-16 01:05:32,452 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:33,686 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:34,617 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:35,875 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:36,379 - [INFO] - [Epoch 17 valid]: MRR: Tail : 0.39682, Head : 0.1954, Avg : 0.29611\n","2023-03-16 01:05:36,491 - [INFO] - [Epoch 17]: Training Loss: 0.0016983, Valid MRR: 0.29611\n","\n","\n","2023-03-16 01:05:36,969 - [INFO] - [E:18| 0]: Train Loss:0.0015597,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:39,417 - [INFO] - [E:18| 100]: Train Loss:0.0016935,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:41,901 - [INFO] - [E:18| 200]: Train Loss:0.0017045,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:44,408 - [INFO] - [E:18| 300]: Train Loss:0.0017013,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:46,888 - [INFO] - [E:18| 400]: Train Loss:0.001705,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:49,343 - [INFO] - [E:18| 500]: Train Loss:0.0017022,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:51,809 - [INFO] - [E:18| 600]: Train Loss:0.0016983,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:54,275 - [INFO] - [E:18| 700]: Train Loss:0.001694,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:56,766 - [INFO] - [E:18| 800]: Train Loss:0.0016905,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:05:59,219 - [INFO] - [E:18| 900]: Train Loss:0.0016867,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:01,699 - [INFO] - [E:18| 1000]: Train Loss:0.0016908,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:04,152 - [INFO] - [E:18| 1100]: Train Loss:0.001687,  Val MRR:0.29611\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:05,929 - [INFO] - [Epoch:18]:  Training Loss:0.001684\n","\n","2023-03-16 01:06:06,384 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:07,605 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:08,579 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:09,794 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:10,296 - [INFO] - [Epoch 18 valid]: MRR: Tail : 0.39831, Head : 0.2002, Avg : 0.29926\n","2023-03-16 01:06:10,415 - [INFO] - [Epoch 18]: Training Loss: 0.0016843, Valid MRR: 0.29926\n","\n","\n","2023-03-16 01:06:10,890 - [INFO] - [E:19| 0]: Train Loss:0.0046017,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:13,381 - [INFO] - [E:19| 100]: Train Loss:0.00168,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:15,843 - [INFO] - [E:19| 200]: Train Loss:0.0016611,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:18,294 - [INFO] - [E:19| 300]: Train Loss:0.0016574,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:20,766 - [INFO] - [E:19| 400]: Train Loss:0.0016497,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:23,265 - [INFO] - [E:19| 500]: Train Loss:0.0016594,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:25,751 - [INFO] - [E:19| 600]: Train Loss:0.0016664,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:28,229 - [INFO] - [E:19| 700]: Train Loss:0.0016668,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:30,734 - [INFO] - [E:19| 800]: Train Loss:0.0016658,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:33,215 - [INFO] - [E:19| 900]: Train Loss:0.0016659,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:35,685 - [INFO] - [E:19| 1000]: Train Loss:0.0016672,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:38,155 - [INFO] - [E:19| 1100]: Train Loss:0.0016706,  Val MRR:0.29926\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:39,934 - [INFO] - [Epoch:19]:  Training Loss:0.001668\n","\n","2023-03-16 01:06:40,357 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:41,587 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:42,534 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:43,804 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:44,297 - [INFO] - [Epoch 19 valid]: MRR: Tail : 0.40362, Head : 0.20488, Avg : 0.30425\n","2023-03-16 01:06:44,415 - [INFO] - [Epoch 19]: Training Loss: 0.0016676, Valid MRR: 0.30425\n","\n","\n","2023-03-16 01:06:44,876 - [INFO] - [E:20| 0]: Train Loss:0.0016021,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:47,373 - [INFO] - [E:20| 100]: Train Loss:0.001596,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:49,868 - [INFO] - [E:20| 200]: Train Loss:0.00162,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:52,346 - [INFO] - [E:20| 300]: Train Loss:0.0016488,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:54,797 - [INFO] - [E:20| 400]: Train Loss:0.0016574,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:57,265 - [INFO] - [E:20| 500]: Train Loss:0.0016603,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:06:59,768 - [INFO] - [E:20| 600]: Train Loss:0.0016583,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:02,232 - [INFO] - [E:20| 700]: Train Loss:0.0016518,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:04,717 - [INFO] - [E:20| 800]: Train Loss:0.0016515,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:07,173 - [INFO] - [E:20| 900]: Train Loss:0.0016539,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:09,666 - [INFO] - [E:20| 1000]: Train Loss:0.0016541,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:12,159 - [INFO] - [E:20| 1100]: Train Loss:0.0016513,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:13,975 - [INFO] - [Epoch:20]:  Training Loss:0.001653\n","\n","2023-03-16 01:07:14,428 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:15,627 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:16,614 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:17,822 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:18,315 - [INFO] - [Epoch 20 valid]: MRR: Tail : 0.39902, Head : 0.20254, Avg : 0.30078\n","2023-03-16 01:07:18,375 - [INFO] - [Epoch 20]: Training Loss: 0.0016535, Valid MRR: 0.30425\n","\n","\n","2023-03-16 01:07:18,858 - [INFO] - [E:21| 0]: Train Loss:0.0014833,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:21,312 - [INFO] - [E:21| 100]: Train Loss:0.0016499,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:23,768 - [INFO] - [E:21| 200]: Train Loss:0.0016719,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:26,227 - [INFO] - [E:21| 300]: Train Loss:0.0016697,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:28,693 - [INFO] - [E:21| 400]: Train Loss:0.0016553,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:31,168 - [INFO] - [E:21| 500]: Train Loss:0.0016464,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:33,635 - [INFO] - [E:21| 600]: Train Loss:0.0016538,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:36,122 - [INFO] - [E:21| 700]: Train Loss:0.001652,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:38,613 - [INFO] - [E:21| 800]: Train Loss:0.0016482,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:41,093 - [INFO] - [E:21| 900]: Train Loss:0.0016456,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:43,591 - [INFO] - [E:21| 1000]: Train Loss:0.0016411,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:46,062 - [INFO] - [E:21| 1100]: Train Loss:0.0016444,  Val MRR:0.30425\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:47,856 - [INFO] - [Epoch:21]:  Training Loss:0.001643\n","\n","2023-03-16 01:07:48,278 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:49,525 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:50,491 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:51,714 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:52,231 - [INFO] - [Epoch 21 valid]: MRR: Tail : 0.40933, Head : 0.20586, Avg : 0.30759\n","2023-03-16 01:07:52,335 - [INFO] - [Epoch 21]: Training Loss: 0.0016434, Valid MRR: 0.30759\n","\n","\n","2023-03-16 01:07:52,809 - [INFO] - [E:22| 0]: Train Loss:0.002654,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:55,280 - [INFO] - [E:22| 100]: Train Loss:0.0016237,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:07:57,751 - [INFO] - [E:22| 200]: Train Loss:0.0016212,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:00,267 - [INFO] - [E:22| 300]: Train Loss:0.0016197,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:02,769 - [INFO] - [E:22| 400]: Train Loss:0.0016156,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:05,255 - [INFO] - [E:22| 500]: Train Loss:0.0016196,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:07,745 - [INFO] - [E:22| 600]: Train Loss:0.0016225,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:10,228 - [INFO] - [E:22| 700]: Train Loss:0.001624,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:12,718 - [INFO] - [E:22| 800]: Train Loss:0.0016285,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:15,194 - [INFO] - [E:22| 900]: Train Loss:0.0016288,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:17,666 - [INFO] - [E:22| 1000]: Train Loss:0.0016318,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:20,124 - [INFO] - [E:22| 1100]: Train Loss:0.0016318,  Val MRR:0.30759\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:21,898 - [INFO] - [Epoch:22]:  Training Loss:0.001637\n","\n","2023-03-16 01:08:22,344 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:23,579 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:24,559 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:25,800 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:26,303 - [INFO] - [Epoch 22 valid]: MRR: Tail : 0.41043, Head : 0.20828, Avg : 0.30935\n","2023-03-16 01:08:26,426 - [INFO] - [Epoch 22]: Training Loss: 0.0016369, Valid MRR: 0.30935\n","\n","\n","2023-03-16 01:08:26,911 - [INFO] - [E:23| 0]: Train Loss:0.0015789,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:29,400 - [INFO] - [E:23| 100]: Train Loss:0.0015951,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:31,860 - [INFO] - [E:23| 200]: Train Loss:0.0015968,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:34,344 - [INFO] - [E:23| 300]: Train Loss:0.0015995,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:36,823 - [INFO] - [E:23| 400]: Train Loss:0.0016413,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:39,335 - [INFO] - [E:23| 500]: Train Loss:0.0016393,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:41,796 - [INFO] - [E:23| 600]: Train Loss:0.0016376,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:44,287 - [INFO] - [E:23| 700]: Train Loss:0.0016297,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:46,742 - [INFO] - [E:23| 800]: Train Loss:0.0016329,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:49,212 - [INFO] - [E:23| 900]: Train Loss:0.0016311,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:51,678 - [INFO] - [E:23| 1000]: Train Loss:0.0016316,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:54,148 - [INFO] - [E:23| 1100]: Train Loss:0.0016346,  Val MRR:0.30935\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:55,941 - [INFO] - [Epoch:23]:  Training Loss:0.001637\n","\n","2023-03-16 01:08:56,380 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:57,620 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:58,542 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:08:59,764 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:00,263 - [INFO] - [Epoch 23 valid]: MRR: Tail : 0.40788, Head : 0.21282, Avg : 0.31035\n","2023-03-16 01:09:00,379 - [INFO] - [Epoch 23]: Training Loss: 0.0016375, Valid MRR: 0.31035\n","\n","\n","2023-03-16 01:09:00,868 - [INFO] - [E:24| 0]: Train Loss:0.0014698,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:03,355 - [INFO] - [E:24| 100]: Train Loss:0.00162,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:05,824 - [INFO] - [E:24| 200]: Train Loss:0.0016194,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:08,295 - [INFO] - [E:24| 300]: Train Loss:0.0016291,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:10,794 - [INFO] - [E:24| 400]: Train Loss:0.0016325,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:13,260 - [INFO] - [E:24| 500]: Train Loss:0.0016337,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:15,723 - [INFO] - [E:24| 600]: Train Loss:0.0016281,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:18,195 - [INFO] - [E:24| 700]: Train Loss:0.0016231,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:20,650 - [INFO] - [E:24| 800]: Train Loss:0.0016256,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:23,124 - [INFO] - [E:24| 900]: Train Loss:0.00162,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:25,610 - [INFO] - [E:24| 1000]: Train Loss:0.0016186,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:28,082 - [INFO] - [E:24| 1100]: Train Loss:0.0016156,  Val MRR:0.31035\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:29,863 - [INFO] - [Epoch:24]:  Training Loss:0.001618\n","\n","2023-03-16 01:09:30,290 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:31,545 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:32,491 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:33,769 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:34,266 - [INFO] - [Epoch 24 valid]: MRR: Tail : 0.41057, Head : 0.21017, Avg : 0.31037\n","2023-03-16 01:09:34,370 - [INFO] - [Epoch 24]: Training Loss: 0.0016178, Valid MRR: 0.31037\n","\n","\n","2023-03-16 01:09:34,857 - [INFO] - [E:25| 0]: Train Loss:0.0017483,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:37,327 - [INFO] - [E:25| 100]: Train Loss:0.0016124,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:39,804 - [INFO] - [E:25| 200]: Train Loss:0.0016124,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:42,276 - [INFO] - [E:25| 300]: Train Loss:0.0016104,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:44,721 - [INFO] - [E:25| 400]: Train Loss:0.0016002,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:47,188 - [INFO] - [E:25| 500]: Train Loss:0.0015912,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:49,650 - [INFO] - [E:25| 600]: Train Loss:0.0015951,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:52,121 - [INFO] - [E:25| 700]: Train Loss:0.0016085,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:54,633 - [INFO] - [E:25| 800]: Train Loss:0.0016113,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:57,148 - [INFO] - [E:25| 900]: Train Loss:0.0016123,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:09:59,648 - [INFO] - [E:25| 1000]: Train Loss:0.0016145,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:02,126 - [INFO] - [E:25| 1100]: Train Loss:0.0016132,  Val MRR:0.31037\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:03,927 - [INFO] - [Epoch:25]:  Training Loss:0.001613\n","\n","2023-03-16 01:10:04,391 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:05,645 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:06,601 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:07,827 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:08,329 - [INFO] - [Epoch 25 valid]: MRR: Tail : 0.41382, Head : 0.21142, Avg : 0.31262\n","2023-03-16 01:10:08,442 - [INFO] - [Epoch 25]: Training Loss: 0.0016131, Valid MRR: 0.31262\n","\n","\n","2023-03-16 01:10:08,914 - [INFO] - [E:26| 0]: Train Loss:0.0016624,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:11,383 - [INFO] - [E:26| 100]: Train Loss:0.0016092,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:13,839 - [INFO] - [E:26| 200]: Train Loss:0.0015903,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:16,299 - [INFO] - [E:26| 300]: Train Loss:0.0015876,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:18,798 - [INFO] - [E:26| 400]: Train Loss:0.0015882,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:21,314 - [INFO] - [E:26| 500]: Train Loss:0.0015925,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:23,786 - [INFO] - [E:26| 600]: Train Loss:0.0015846,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:26,250 - [INFO] - [E:26| 700]: Train Loss:0.0015853,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:28,694 - [INFO] - [E:26| 800]: Train Loss:0.0015986,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:31,187 - [INFO] - [E:26| 900]: Train Loss:0.0015956,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:33,694 - [INFO] - [E:26| 1000]: Train Loss:0.0015903,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:36,247 - [INFO] - [E:26| 1100]: Train Loss:0.0015983,  Val MRR:0.31262\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:38,056 - [INFO] - [Epoch:26]:  Training Loss:0.001599\n","\n","2023-03-16 01:10:38,474 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:39,695 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:40,666 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:41,899 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:42,399 - [INFO] - [Epoch 26 valid]: MRR: Tail : 0.41398, Head : 0.21371, Avg : 0.31384\n","2023-03-16 01:10:42,511 - [INFO] - [Epoch 26]: Training Loss: 0.0015985, Valid MRR: 0.31384\n","\n","\n","2023-03-16 01:10:42,965 - [INFO] - [E:27| 0]: Train Loss:0.0017386,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:45,438 - [INFO] - [E:27| 100]: Train Loss:0.0016194,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:47,909 - [INFO] - [E:27| 200]: Train Loss:0.0016256,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:50,365 - [INFO] - [E:27| 300]: Train Loss:0.0016123,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:52,823 - [INFO] - [E:27| 400]: Train Loss:0.0016053,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:55,283 - [INFO] - [E:27| 500]: Train Loss:0.0015999,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:10:57,732 - [INFO] - [E:27| 600]: Train Loss:0.0015975,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:00,193 - [INFO] - [E:27| 700]: Train Loss:0.0015981,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:02,683 - [INFO] - [E:27| 800]: Train Loss:0.0015929,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:05,175 - [INFO] - [E:27| 900]: Train Loss:0.001592,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:07,667 - [INFO] - [E:27| 1000]: Train Loss:0.0015926,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:10,129 - [INFO] - [E:27| 1100]: Train Loss:0.0015948,  Val MRR:0.31384\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:11,911 - [INFO] - [Epoch:27]:  Training Loss:0.001599\n","\n","2023-03-16 01:11:12,346 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:13,568 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:14,508 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:15,757 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:16,251 - [INFO] - [Epoch 27 valid]: MRR: Tail : 0.41288, Head : 0.21821, Avg : 0.31555\n","2023-03-16 01:11:16,373 - [INFO] - [Epoch 27]: Training Loss: 0.0015989, Valid MRR: 0.31555\n","\n","\n","2023-03-16 01:11:16,854 - [INFO] - [E:28| 0]: Train Loss:0.0017006,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:19,325 - [INFO] - [E:28| 100]: Train Loss:0.0015847,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:21,824 - [INFO] - [E:28| 200]: Train Loss:0.0015785,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:24,303 - [INFO] - [E:28| 300]: Train Loss:0.0015829,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:26,747 - [INFO] - [E:28| 400]: Train Loss:0.0015785,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:29,230 - [INFO] - [E:28| 500]: Train Loss:0.0015776,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:31,684 - [INFO] - [E:28| 600]: Train Loss:0.0015849,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:34,160 - [INFO] - [E:28| 700]: Train Loss:0.0015848,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:36,642 - [INFO] - [E:28| 800]: Train Loss:0.001587,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:39,122 - [INFO] - [E:28| 900]: Train Loss:0.0015835,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:41,609 - [INFO] - [E:28| 1000]: Train Loss:0.0015889,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:44,109 - [INFO] - [E:28| 1100]: Train Loss:0.0015893,  Val MRR:0.31555\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:45,905 - [INFO] - [Epoch:28]:  Training Loss:0.001589\n","\n","2023-03-16 01:11:46,321 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:47,555 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:48,494 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:49,728 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:50,225 - [INFO] - [Epoch 28 valid]: MRR: Tail : 0.41801, Head : 0.21936, Avg : 0.31868\n","2023-03-16 01:11:50,334 - [INFO] - [Epoch 28]: Training Loss: 0.0015893, Valid MRR: 0.31868\n","\n","\n","2023-03-16 01:11:50,807 - [INFO] - [E:29| 0]: Train Loss:0.0014635,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:53,295 - [INFO] - [E:29| 100]: Train Loss:0.0015386,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:55,766 - [INFO] - [E:29| 200]: Train Loss:0.0015915,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:11:58,244 - [INFO] - [E:29| 300]: Train Loss:0.0016007,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:00,713 - [INFO] - [E:29| 400]: Train Loss:0.0015962,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:03,205 - [INFO] - [E:29| 500]: Train Loss:0.0015858,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:05,679 - [INFO] - [E:29| 600]: Train Loss:0.0015864,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:08,156 - [INFO] - [E:29| 700]: Train Loss:0.0015836,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:10,659 - [INFO] - [E:29| 800]: Train Loss:0.0015831,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:13,151 - [INFO] - [E:29| 900]: Train Loss:0.0015857,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:15,626 - [INFO] - [E:29| 1000]: Train Loss:0.0015882,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:18,111 - [INFO] - [E:29| 1100]: Train Loss:0.0015863,  Val MRR:0.31868\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:19,896 - [INFO] - [Epoch:29]:  Training Loss:0.001584\n","\n","2023-03-16 01:12:20,341 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:21,568 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:22,551 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:23,822 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:24,323 - [INFO] - [Epoch 29 valid]: MRR: Tail : 0.41642, Head : 0.22202, Avg : 0.31922\n","2023-03-16 01:12:24,435 - [INFO] - [Epoch 29]: Training Loss: 0.0015845, Valid MRR: 0.31922\n","\n","\n","2023-03-16 01:12:24,901 - [INFO] - [E:30| 0]: Train Loss:0.0014989,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:27,384 - [INFO] - [E:30| 100]: Train Loss:0.0015426,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:29,879 - [INFO] - [E:30| 200]: Train Loss:0.0015858,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:32,373 - [INFO] - [E:30| 300]: Train Loss:0.0015818,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:34,842 - [INFO] - [E:30| 400]: Train Loss:0.0015871,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:37,333 - [INFO] - [E:30| 500]: Train Loss:0.0015891,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:39,814 - [INFO] - [E:30| 600]: Train Loss:0.0015955,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:42,316 - [INFO] - [E:30| 700]: Train Loss:0.0015924,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:44,822 - [INFO] - [E:30| 800]: Train Loss:0.0015862,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:47,291 - [INFO] - [E:30| 900]: Train Loss:0.0015823,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:49,770 - [INFO] - [E:30| 1000]: Train Loss:0.0015782,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:52,244 - [INFO] - [E:30| 1100]: Train Loss:0.0015762,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:54,028 - [INFO] - [Epoch:30]:  Training Loss:0.001575\n","\n","2023-03-16 01:12:54,467 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:55,703 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:56,655 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:57,907 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:12:58,413 - [INFO] - [Epoch 30 valid]: MRR: Tail : 0.41787, Head : 0.2181, Avg : 0.31798\n","2023-03-16 01:12:58,459 - [INFO] - [Epoch 30]: Training Loss: 0.0015747, Valid MRR: 0.31922\n","\n","\n","2023-03-16 01:12:58,943 - [INFO] - [E:31| 0]: Train Loss:0.0015679,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:01,451 - [INFO] - [E:31| 100]: Train Loss:0.0015445,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:03,954 - [INFO] - [E:31| 200]: Train Loss:0.0015587,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:06,448 - [INFO] - [E:31| 300]: Train Loss:0.0015485,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:08,925 - [INFO] - [E:31| 400]: Train Loss:0.0015513,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:11,389 - [INFO] - [E:31| 500]: Train Loss:0.0015475,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:13,865 - [INFO] - [E:31| 600]: Train Loss:0.0015523,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:16,350 - [INFO] - [E:31| 700]: Train Loss:0.0015601,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:18,844 - [INFO] - [E:31| 800]: Train Loss:0.0015609,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:21,323 - [INFO] - [E:31| 900]: Train Loss:0.0015626,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:23,779 - [INFO] - [E:31| 1000]: Train Loss:0.0015597,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:26,254 - [INFO] - [E:31| 1100]: Train Loss:0.0015626,  Val MRR:0.31922\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:28,027 - [INFO] - [Epoch:31]:  Training Loss:0.001567\n","\n","2023-03-16 01:13:28,460 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:29,687 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:30,645 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:31,926 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:32,435 - [INFO] - [Epoch 31 valid]: MRR: Tail : 0.42079, Head : 0.21953, Avg : 0.32016\n","2023-03-16 01:13:32,555 - [INFO] - [Epoch 31]: Training Loss: 0.0015671, Valid MRR: 0.32016\n","\n","\n","2023-03-16 01:13:33,044 - [INFO] - [E:32| 0]: Train Loss:0.0015067,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:35,526 - [INFO] - [E:32| 100]: Train Loss:0.0015499,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:37,978 - [INFO] - [E:32| 200]: Train Loss:0.0015608,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:40,446 - [INFO] - [E:32| 300]: Train Loss:0.0015729,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:42,929 - [INFO] - [E:32| 400]: Train Loss:0.001571,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:45,413 - [INFO] - [E:32| 500]: Train Loss:0.0015649,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:47,901 - [INFO] - [E:32| 600]: Train Loss:0.0015658,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:50,373 - [INFO] - [E:32| 700]: Train Loss:0.0015607,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:52,870 - [INFO] - [E:32| 800]: Train Loss:0.0015679,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:55,335 - [INFO] - [E:32| 900]: Train Loss:0.0015712,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:13:57,794 - [INFO] - [E:32| 1000]: Train Loss:0.00157,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:00,274 - [INFO] - [E:32| 1100]: Train Loss:0.0015701,  Val MRR:0.32016\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:02,067 - [INFO] - [Epoch:32]:  Training Loss:0.001568\n","\n","2023-03-16 01:14:02,505 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:03,739 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:04,677 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:05,899 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:06,411 - [INFO] - [Epoch 32 valid]: MRR: Tail : 0.4194, Head : 0.22252, Avg : 0.32096\n","2023-03-16 01:14:06,526 - [INFO] - [Epoch 32]: Training Loss: 0.0015678, Valid MRR: 0.32096\n","\n","\n","2023-03-16 01:14:07,016 - [INFO] - [E:33| 0]: Train Loss:0.0014379,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:09,480 - [INFO] - [E:33| 100]: Train Loss:0.0015112,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:11,957 - [INFO] - [E:33| 200]: Train Loss:0.0015323,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:14,420 - [INFO] - [E:33| 300]: Train Loss:0.0015322,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:16,907 - [INFO] - [E:33| 400]: Train Loss:0.001545,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:19,381 - [INFO] - [E:33| 500]: Train Loss:0.0015453,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:21,859 - [INFO] - [E:33| 600]: Train Loss:0.0015547,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:24,323 - [INFO] - [E:33| 700]: Train Loss:0.0015529,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:26,820 - [INFO] - [E:33| 800]: Train Loss:0.001552,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:29,289 - [INFO] - [E:33| 900]: Train Loss:0.0015579,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:31,774 - [INFO] - [E:33| 1000]: Train Loss:0.0015558,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:34,282 - [INFO] - [E:33| 1100]: Train Loss:0.0015569,  Val MRR:0.32096\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:36,064 - [INFO] - [Epoch:33]:  Training Loss:0.001557\n","\n","2023-03-16 01:14:36,508 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:37,750 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:38,724 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:39,939 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:40,448 - [INFO] - [Epoch 33 valid]: MRR: Tail : 0.42378, Head : 0.22335, Avg : 0.32357\n","2023-03-16 01:14:40,552 - [INFO] - [Epoch 33]: Training Loss: 0.0015573, Valid MRR: 0.32357\n","\n","\n","2023-03-16 01:14:41,050 - [INFO] - [E:34| 0]: Train Loss:0.0013846,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:43,525 - [INFO] - [E:34| 100]: Train Loss:0.0014893,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:46,022 - [INFO] - [E:34| 200]: Train Loss:0.0015226,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:48,490 - [INFO] - [E:34| 300]: Train Loss:0.0015419,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:50,953 - [INFO] - [E:34| 400]: Train Loss:0.0015549,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:53,416 - [INFO] - [E:34| 500]: Train Loss:0.0015664,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:55,898 - [INFO] - [E:34| 600]: Train Loss:0.0015629,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:14:58,355 - [INFO] - [E:34| 700]: Train Loss:0.0015614,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:00,816 - [INFO] - [E:34| 800]: Train Loss:0.0015568,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:03,320 - [INFO] - [E:34| 900]: Train Loss:0.0015528,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:05,809 - [INFO] - [E:34| 1000]: Train Loss:0.0015496,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:08,303 - [INFO] - [E:34| 1100]: Train Loss:0.0015525,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:10,090 - [INFO] - [Epoch:34]:  Training Loss:0.001554\n","\n","2023-03-16 01:15:10,518 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:11,768 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:12,718 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:13,962 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:14,461 - [INFO] - [Epoch 34 valid]: MRR: Tail : 0.42148, Head : 0.22386, Avg : 0.32267\n","2023-03-16 01:15:14,519 - [INFO] - [Epoch 34]: Training Loss: 0.001554, Valid MRR: 0.32357\n","\n","\n","2023-03-16 01:15:15,005 - [INFO] - [E:35| 0]: Train Loss:0.0014375,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:17,493 - [INFO] - [E:35| 100]: Train Loss:0.0015741,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:19,977 - [INFO] - [E:35| 200]: Train Loss:0.0015632,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:22,449 - [INFO] - [E:35| 300]: Train Loss:0.0015611,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:24,922 - [INFO] - [E:35| 400]: Train Loss:0.0015583,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:27,445 - [INFO] - [E:35| 500]: Train Loss:0.0015546,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:29,937 - [INFO] - [E:35| 600]: Train Loss:0.0015559,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:32,398 - [INFO] - [E:35| 700]: Train Loss:0.0015519,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:34,877 - [INFO] - [E:35| 800]: Train Loss:0.0015477,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:37,363 - [INFO] - [E:35| 900]: Train Loss:0.0015424,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:39,831 - [INFO] - [E:35| 1000]: Train Loss:0.0015485,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:42,312 - [INFO] - [E:35| 1100]: Train Loss:0.0015496,  Val MRR:0.32357\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:44,100 - [INFO] - [Epoch:35]:  Training Loss:0.001548\n","\n","2023-03-16 01:15:44,541 - [INFO] - [Valid, Tail_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:45,774 - [INFO] - [Valid, Tail_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:46,721 - [INFO] - [Valid, Head_Batch Step 0]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:47,957 - [INFO] - [Valid, Head_Batch Step 100]\tFB_16_03_2023_00:55:18\n","2023-03-16 01:15:48,463 - [INFO] - [Epoch 35 valid]: MRR: Tail : 0.42606, Head : 0.22655, Avg : 0.32631\n","2023-03-16 01:15:48,586 - [INFO] - [Epoch 35]: Training Loss: 0.0015475, Valid MRR: 0.32631\n","\n","\n","2023-03-16 01:15:49,075 - [INFO] - [E:36| 0]: Train Loss:0.0014684,  Val MRR:0.32631\tFB_16_03_2023_00:55:18\n"]}]},{"cell_type":"code","source":["!python run.py -score_func distmult -opn mult -gcn_layer 3 -gpu 0 -data WN18RR -name WN \\\n","    -logdir '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/log/' \\\n","    -config '/content/drive/MyDrive/Colab Notebooks/partC/CompGCN_Neptune/config/'"],"metadata":{"id":"RAWefPl4km_P"},"execution_count":null,"outputs":[]}]}